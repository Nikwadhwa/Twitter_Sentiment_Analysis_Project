{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Twitter Sentiment Analysis Project ðŸŽ¯**\n",
        "\n",
        "Welcome to this Twitter Sentiment Analysis project! ðŸŒŸ This notebook demonstrates how to analyze the sentiments of tweets, classifying them into categories like positive, negative, or neutral. By leveraging the power of NLP (Natural Language Processing) techniques and machine learning models, we'll uncover patterns and insights from social media conversations.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WXzQ_Z5pUuRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Highlights:\n",
        "Dataset: Real-world tweets collected from Twitter.\n",
        "Objective: Analyze sentiments to understand public opinion on a topic or event.\n",
        "Tech Stack: Python, Pandas, NLTK, Scikit-learn, and more.\n",
        "Steps:\n",
        "Data collection and preprocessing (removing noise, tokenization, etc.).\n",
        "Sentiment classification using machine learning algorithms.**"
      ],
      "metadata": {
        "id": "EdEXBYi2VZoF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D9QgurFuhB7"
      },
      "source": [
        "#### Import necessary packages\n",
        "You may import more packages here."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfbEgsHDuoIz",
        "outputId": "44dbb331-f27a-425c-d5a9-2801004a394a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (2.10.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras\n",
        "!pip install tensorflow==2.12\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suZFfjnSacQ9",
        "outputId": "90a4b970-8108-4135-caa1-93673af7419d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (2.12.0)\n",
            "Requirement already satisfied: tensorflow==2.12 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (2.12.0)\n",
            "Requirement already satisfied: tensorflow-intel==2.12.0 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorflow==2.12) (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12) (24.3.7)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12) (3.10.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12) (0.4.25)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12) (3.3.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12) (4.25.3)\n",
            "Requirement already satisfied: setuptools in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12) (68.0.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12) (2.12.0)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12) (2.12.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12) (0.31.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow==2.12) (0.38.4)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12) (0.3.2)\n",
            "Requirement already satisfied: scipy>=1.9 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12) (1.11.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12) (2.29.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12) (2.2.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12) (1.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12) (2.1.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\nikhil wadhwa\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooTEYb79uhB8"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "import re\n",
        "from os.path import join\n",
        "import numpy as np\n",
        "import emoji\n",
        "import os\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from copy import deepcopy\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtext.vocab import Vectors\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGrvSmjeuhB9"
      },
      "outputs": [],
      "source": [
        "# Define test sets\n",
        "testsets = ['twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6vnyIu7uhB9"
      },
      "outputs": [],
      "source": [
        "# Skeleton: Evaluation code for the test sets\n",
        "def read_test(testset):\n",
        "    '''\n",
        "    readin the testset and return a dictionary\n",
        "    :param testset: str, the file name of the testset to compare\n",
        "    '''\n",
        "    id_gts = {}\n",
        "    with open(testset, 'r', encoding='utf8') as fh:\n",
        "        for line in fh:\n",
        "            fields = line.split('\\t')\n",
        "            tweetid = fields[0]\n",
        "            gt = fields[1]\n",
        "\n",
        "            id_gts[tweetid] = gt\n",
        "\n",
        "    return id_gts\n",
        "\n",
        "\n",
        "def confusion(id_preds, testset, classifier):\n",
        "    '''\n",
        "    print the confusion matrix of {'positive', 'netative'} between preds and testset\n",
        "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
        "    :param testset: str, the file name of the testset to compare\n",
        "    :classifier: str, the name of the classifier\n",
        "    '''\n",
        "    id_gts = read_test(testset)\n",
        "\n",
        "    gts = []\n",
        "    for m, c1 in id_gts.items():\n",
        "        if c1 not in gts:\n",
        "            gts.append(c1)\n",
        "\n",
        "    gts = ['positive', 'negative', 'neutral']\n",
        "\n",
        "    conf = {}\n",
        "    for c1 in gts:\n",
        "        conf[c1] = {}\n",
        "        for c2 in gts:\n",
        "            conf[c1][c2] = 0\n",
        "\n",
        "    for tweetid, gt in id_gts.items():\n",
        "        if tweetid in id_preds:\n",
        "            pred = id_preds[tweetid]\n",
        "        else:\n",
        "            pred = 'neutral'\n",
        "        conf[pred][gt] += 1\n",
        "\n",
        "    print(''.ljust(12) + '  '.join(gts))\n",
        "\n",
        "    for c1 in gts:\n",
        "        print(c1.ljust(12), end='')\n",
        "        for c2 in gts:\n",
        "            if sum(conf[c1].values()) > 0:\n",
        "                print('%.3f     ' % (conf[c1][c2] / float(sum(conf[c1].values()))), end='')\n",
        "            else:\n",
        "                print('0.000     ', end='')\n",
        "        print('')\n",
        "\n",
        "    print('')\n",
        "\n",
        "\n",
        "def evaluate(id_preds, testset, classifier):\n",
        "    '''\n",
        "    print the macro-F1 score of {'positive', 'netative'} between preds and testset\n",
        "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
        "    :param testset: str, the file name of the testset to compare\n",
        "    :classifier: str, the name of the classifier\n",
        "    '''\n",
        "    id_gts = read_test(testset)\n",
        "\n",
        "    acc_by_class = {}\n",
        "    for gt in ['positive', 'negative', 'neutral']:\n",
        "        acc_by_class[gt] = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
        "\n",
        "    catf1s = {}\n",
        "\n",
        "    ok = 0\n",
        "    for tweetid, gt in id_gts.items():\n",
        "        if tweetid in id_preds:\n",
        "            pred = id_preds[tweetid]\n",
        "        else:\n",
        "            pred = 'neutral'\n",
        "\n",
        "        if gt == pred:\n",
        "            ok += 1\n",
        "            acc_by_class[gt]['tp'] += 1\n",
        "        else:\n",
        "            acc_by_class[gt]['fn'] += 1\n",
        "            acc_by_class[pred]['fp'] += 1\n",
        "\n",
        "    catcount = 0\n",
        "    itemcount = 0\n",
        "    macro = {'p': 0, 'r': 0, 'f1': 0}\n",
        "    micro = {'p': 0, 'r': 0, 'f1': 0}\n",
        "    semevalmacro = {'p': 0, 'r': 0, 'f1': 0}\n",
        "\n",
        "    microtp = 0\n",
        "    microfp = 0\n",
        "    microtn = 0\n",
        "    microfn = 0\n",
        "    for cat, acc in acc_by_class.items():\n",
        "        catcount += 1\n",
        "\n",
        "        microtp += acc['tp']\n",
        "        microfp += acc['fp']\n",
        "        microtn += acc['tn']\n",
        "        microfn += acc['fn']\n",
        "\n",
        "        p = 0\n",
        "        if (acc['tp'] + acc['fp']) > 0:\n",
        "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
        "\n",
        "        r = 0\n",
        "        if (acc['tp'] + acc['fn']) > 0:\n",
        "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
        "\n",
        "        f1 = 0\n",
        "        if (p + r) > 0:\n",
        "            f1 = 2 * p * r / (p + r)\n",
        "\n",
        "        catf1s[cat] = f1\n",
        "\n",
        "        n = acc['tp'] + acc['fn']\n",
        "\n",
        "        macro['p'] += p\n",
        "        macro['r'] += r\n",
        "        macro['f1'] += f1\n",
        "\n",
        "        if cat in ['positive', 'negative']:\n",
        "            semevalmacro['p'] += p\n",
        "            semevalmacro['r'] += r\n",
        "            semevalmacro['f1'] += f1\n",
        "\n",
        "        itemcount += n\n",
        "\n",
        "    micro['p'] = float(microtp) / float(microtp + microfp)\n",
        "    micro['r'] = float(microtp) / float(microtp + microfn)\n",
        "    micro['f1'] = 2 * float(micro['p']) * micro['r'] / float(micro['p'] + micro['r'])\n",
        "\n",
        "    semevalmacrof1 = semevalmacro['f1'] / 2\n",
        "\n",
        "    print(testset + ' (' + classifier + '): %.3f' % semevalmacrof1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxKilgQFuhB9"
      },
      "source": [
        "#### Load training set, dev set and testing set\n",
        "Here, you need to load the training set, the development set and the test set. For better classification results, you may need to preprocess tweets before sending them to the classifiers."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "unzipping semeval tweets folder"
      ],
      "metadata": {
        "id": "0kT8alpWx2nC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!tar -xvjf /content/semeval-tweets.tar.bz2 -C /content/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJR8w7G-x0we",
        "outputId": "1b231738-9262-473f-f396-ba5501071e69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "semeval-tweets/\n",
            "semeval-tweets/twitter-training-data.txt\n",
            "semeval-tweets/twitter-dev-data.txt\n",
            "semeval-tweets/twitter-test3.txt\n",
            "semeval-tweets/twitter-test2.txt\n",
            "semeval-tweets/twitter-test1.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1JLOS80uhB9"
      },
      "outputs": [],
      "source": [
        "# Load training set, dev set and testing set\n",
        "def load_in_dataframe(file_path):\n",
        "    df = pd.read_csv(file_path, sep='\\t', header=None, names=['Tweet_id', 'Sentiment', 'Tweet'])\n",
        "    return df\n",
        "\n",
        "train_tw_path = os.path.join(os.getcwd(), 'semeval-tweets', 'twitter-training-data.txt')\n",
        "dev_tw_path = os.path.join(os.getcwd(), 'semeval-tweets', 'twitter-dev-data.txt')\n",
        "test1_tw_path = os.path.join(os.getcwd(), 'semeval-tweets', 'twitter-test1.txt')\n",
        "test2_tw_path = os.path.join(os.getcwd(), 'semeval-tweets', 'twitter-test2.txt')\n",
        "test3_tw_path = os.path.join(os.getcwd(), 'semeval-tweets', 'twitter-test3.txt')\n",
        "\n",
        "train_tw_df = load_in_dataframe(train_tw_path)\n",
        "dev_tw_df = load_in_dataframe(dev_tw_path)\n",
        "test1_tw_df = load_in_dataframe(test1_tw_path)\n",
        "test2_tw_df = load_in_dataframe(test2_tw_path)\n",
        "test3_tw_df = load_in_dataframe(test3_tw_path)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA PREPROCESSING"
      ],
      "metadata": {
        "id": "4sOriP_8vfam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading necessary nltk files"
      ],
      "metadata": {
        "id": "dy0Q17bSv_Fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LS9s0xpveuK",
        "outputId": "f258a9f1-7351-4d46-a1c7-684609ed59bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to C:\\Users\\Nikhil\n",
            "[nltk_data]     Wadhwa\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to C:\\Users\\Nikhil\n",
            "[nltk_data]     Wadhwa\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\Nikhil Wadhwa\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to C:\\Users\\Nikhil\n",
            "[nltk_data]     Wadhwa\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a data preprocessing function to improve quality of data extracted and model's accuracy could be improved.**"
      ],
      "metadata": {
        "id": "mqdXEvuxwHbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(file_path):\n",
        "\n",
        "\n",
        "    test_data = pd.read_csv(file_path, delimiter='\\t', header=None, names=['TweetID', 'Sentiment', 'Tweet'])\n",
        "\n",
        "    # Map Sentiment labels\n",
        "    #test_data['Sentiment'] = test_data['Sentiment'].map({'positive': 1, 'negative': -1, 'neutral': 0})\n",
        "\n",
        "    # Lowercase\n",
        "    test_data['Tweet'] = test_data['Tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "\n",
        "    # Remove URLs\n",
        "    test_data['Tweet'] = test_data['Tweet'].apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))\n",
        "\n",
        "    # Replace mentions but keep hashtags\n",
        "    test_data['Tweet'] = test_data['Tweet'].apply(lambda x: re.sub(r'@\\w+', '@user', x))\n",
        "\n",
        "    # Convert emojis to text\n",
        "    test_data['Tweet'] = test_data['Tweet'].apply(lambda x: emoji.demojize(x, delimiters=(\"\", \"\")))\n",
        "\n",
        "    # Remove non-alphanumeric characters except spaces\n",
        "    test_data['Tweet'] = test_data['Tweet'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "\n",
        "    def remove_punctuation(text):\n",
        "    # This regular expression matches any punctuation character\n",
        "      return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    def remove_only_digits(text):\n",
        "    # This regular expression matches whole numbers\n",
        "      return re.sub(r'\\b\\d+\\b', '', text)\n",
        "    test_data['Tweet'] = test_data['Tweet'].apply(remove_punctuation)\n",
        "    test_data['Tweet'] = test_data['Tweet'].apply(remove_only_digits)\n",
        "\n",
        "\n",
        "\n",
        "    # Tokenization\n",
        "    test_data['Tokens'] = test_data['Tweet'].apply(word_tokenize)\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "\n",
        "    # Lemmatize with POS tagging\n",
        "    def get_wordnet_pos(tag):\n",
        "        \"\"\"Convert POS tag from Penn to WordNet.\"\"\"\n",
        "        if tag.startswith('J'):\n",
        "            return wordnet.ADJ\n",
        "        elif tag.startswith('V'):\n",
        "            return wordnet.VERB\n",
        "        elif tag.startswith('N'):\n",
        "            return wordnet.NOUN\n",
        "        elif tag.startswith('R'):\n",
        "            return wordnet.ADV\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def lemmatize_with_pos(tokens):\n",
        "        tagged = nltk.pos_tag(tokens)\n",
        "        lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(tag) if get_wordnet_pos(tag) else wordnet.NOUN) for word, tag in tagged if len(word) > 1 and not word.isdigit()]\n",
        "        return lemmatized\n",
        "\n",
        "    test_data['Lemmatized'] = test_data['Tokens'].apply(lemmatize_with_pos)\n",
        "\n",
        "    return test_data"
      ],
      "metadata": {
        "id": "lZsBwyDpwDsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applying preprocessing function to data**"
      ],
      "metadata": {
        "id": "1TgyjLMqwxZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = preprocess_data(train_tw_path)\n",
        "dev_data = preprocess_data(dev_tw_path)\n",
        "test_data_1 = preprocess_data(test1_tw_path)\n",
        "test_data_2 = preprocess_data(test2_tw_path)\n",
        "test_data_3 = preprocess_data(test3_tw_path)"
      ],
      "metadata": {
        "id": "skAgCRZDwwpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "extracting x_train and y_train from the data , also creating variables for x_dev , y_dev which will be used for validating model."
      ],
      "metadata": {
        "id": "jAAquna40aS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = train_data['Tweet']\n",
        "y_train = train_data['Sentiment']\n",
        "x_dev = dev_data['Tweet']\n",
        "y_dev = dev_data['Sentiment']\n"
      ],
      "metadata": {
        "id": "8VjhPI5e0UX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_datasets = {\n",
        "    test1_tw_path: test_data_1,\n",
        "    test2_tw_path: test_data_2,\n",
        "    test3_tw_path: test_data_3,\n",
        "}"
      ],
      "metadata": {
        "id": "M6EX0pZd1JxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6lHzCXK2PJaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puN18SScuhB-"
      },
      "source": [
        "#### Build sentiment classifiers\n",
        "You need to create your own classifiers (at least 3 classifiers). For each classifier, you can choose between the bag-of-word features and the word-embedding-based features. Each classifier has to be evaluated over 3 test sets. Make sure your classifier produce consistent performance across the test sets. Marking will be based on the performance over all 5 test sets (2 of them are not provided to you)."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5gXPrsDDO3gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoYV8MyKuhB-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcdd1e7e-ad89-4263-d5b0-5913774eca19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training  naive_bayes with bow technique \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.61      0.47      0.53       378\n",
            "     neutral       0.62      0.63      0.63       919\n",
            "    positive       0.62      0.68      0.65       703\n",
            "\n",
            "    accuracy                           0.62      2000\n",
            "   macro avg       0.62      0.59      0.60      2000\n",
            "weighted avg       0.62      0.62      0.62      2000\n",
            "\n",
            "Time to evaluate naive bayes on test sets\n",
            "C:\\Users\\Nikhil Wadhwa\\semeval-tweets\\twitter-test1.txt (naive_bayes): 0.438\n",
            "C:\\Users\\Nikhil Wadhwa\\semeval-tweets\\twitter-test2.txt (naive_bayes): 0.450\n",
            "C:\\Users\\Nikhil Wadhwa\\semeval-tweets\\twitter-test3.txt (naive_bayes): 0.445\n",
            "Training  svm with bow technique \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.54      0.52      0.53       378\n",
            "     neutral       0.60      0.65      0.62       919\n",
            "    positive       0.67      0.62      0.65       703\n",
            "\n",
            "    accuracy                           0.61      2000\n",
            "   macro avg       0.60      0.59      0.60      2000\n",
            "weighted avg       0.61      0.61      0.61      2000\n",
            "\n",
            "Time to evaluate Svm on test sets\n",
            "C:\\Users\\Nikhil Wadhwa\\semeval-tweets\\twitter-test1.txt (svm): 0.511\n",
            "C:\\Users\\Nikhil Wadhwa\\semeval-tweets\\twitter-test2.txt (svm): 0.522\n",
            "C:\\Users\\Nikhil Wadhwa\\semeval-tweets\\twitter-test3.txt (svm): 0.497\n",
            "Training  logistic_regression with bow technique \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.62      0.49      0.55       378\n",
            "     neutral       0.61      0.71      0.66       919\n",
            "    positive       0.69      0.63      0.66       703\n",
            "\n",
            "    accuracy                           0.64      2000\n",
            "   macro avg       0.64      0.61      0.62      2000\n",
            "weighted avg       0.64      0.64      0.64      2000\n",
            "\n",
            "Time to evaluate logistic regression on test sets\n",
            "C:\\Users\\Nikhil Wadhwa\\semeval-tweets\\twitter-test1.txt (logistic_regression): 0.510\n",
            "C:\\Users\\Nikhil Wadhwa\\semeval-tweets\\twitter-test2.txt (logistic_regression): 0.537\n",
            "C:\\Users\\Nikhil Wadhwa\\semeval-tweets\\twitter-test3.txt (logistic_regression): 0.494\n",
            " Training lstm with glove embeddings technique \n",
            "Epoch 1/10\n",
            "1408/1408 [==============================] - 284s 198ms/step - loss: 0.8674 - accuracy: 0.5761 - val_loss: 0.8175 - val_accuracy: 0.5890\n",
            "Epoch 2/10\n",
            "1408/1408 [==============================] - 264s 188ms/step - loss: 0.8087 - accuracy: 0.6153 - val_loss: 0.7814 - val_accuracy: 0.6360\n",
            "Epoch 3/10\n",
            "1408/1408 [==============================] - 267s 190ms/step - loss: 0.7784 - accuracy: 0.6352 - val_loss: 0.7759 - val_accuracy: 0.6355\n",
            "Epoch 4/10\n",
            "1408/1408 [==============================] - 272s 193ms/step - loss: 0.7552 - accuracy: 0.6497 - val_loss: 0.7491 - val_accuracy: 0.6475\n",
            "Epoch 5/10\n",
            "1408/1408 [==============================] - 272s 193ms/step - loss: 0.7355 - accuracy: 0.6628 - val_loss: 0.7411 - val_accuracy: 0.6550\n",
            "Epoch 6/10\n",
            "1408/1408 [==============================] - 270s 192ms/step - loss: 0.7174 - accuracy: 0.6699 - val_loss: 0.7350 - val_accuracy: 0.6610\n",
            "Epoch 7/10\n",
            "1408/1408 [==============================] - 274s 194ms/step - loss: 0.6983 - accuracy: 0.6815 - val_loss: 0.7448 - val_accuracy: 0.6470\n",
            "Epoch 8/10\n",
            "1408/1408 [==============================] - 275s 195ms/step - loss: 0.6815 - accuracy: 0.6906 - val_loss: 0.7466 - val_accuracy: 0.6595\n",
            "Epoch 9/10\n",
            "1408/1408 [==============================] - 281s 199ms/step - loss: 0.6632 - accuracy: 0.7014 - val_loss: 0.7597 - val_accuracy: 0.6460\n",
            "Epoch 10/10\n",
            "1408/1408 [==============================] - 274s 195ms/step - loss: 0.6431 - accuracy: 0.7115 - val_loss: 0.7778 - val_accuracy: 0.6590\n",
            "63/63 [==============================] - 6s 78ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.64      0.51      0.57       378\n",
            "     neutral       0.64      0.72      0.68       919\n",
            "    negative       0.70      0.66      0.68       703\n",
            "\n",
            "    accuracy                           0.66      2000\n",
            "   macro avg       0.66      0.63      0.64      2000\n",
            "weighted avg       0.66      0.66      0.66      2000\n",
            "\n",
            "111/111 [==============================] - 8s 76ms/step\n",
            "C:\\Users\\Nikhil Wadhwa\\semeval-tweets\\twitter-test1.txt (lstm): 0.065\n",
            "58/58 [==============================] - 4s 74ms/step\n",
            "C:\\Users\\Nikhil Wadhwa\\semeval-tweets\\twitter-test2.txt (lstm): 0.062\n",
            "75/75 [==============================] - 6s 75ms/step\n",
            "C:\\Users\\Nikhil Wadhwa\\semeval-tweets\\twitter-test3.txt (lstm): 0.072\n"
          ]
        }
      ],
      "source": [
        "# Buid traditional sentiment classifiers. An example classifier name 'svm' is given\n",
        "# in the code below. You should replace the other two classifier names\n",
        "# with your own choices. For features used for classifier training,\n",
        "# the 'bow' feature is given in the code. But you could also explore the\n",
        "# use of other features.\n",
        "for classifier in ['naive_bayes', 'svm', 'logistic_regression','lstm']:\n",
        "    for features in ['bow']:\n",
        "        if classifier=='naive_bayes' and features=='bow':\n",
        "          print('Training  '+ classifier + ' with ' + features + ' technique ')\n",
        "          naive_bow_pipeline = Pipeline([('vectorizer',CountVectorizer(stop_words='english')),('classifier',MultinomialNB())])\n",
        "          naive_bow_pipeline.fit(x_train, y_train)\n",
        "          #predicting on development set\n",
        "          y_pred_dev = naive_bow_pipeline.predict(x_dev)\n",
        "          print (classification_report(y_dev,y_pred_dev))\n",
        "          print('Time to evaluate naive bayes on test sets')\n",
        "          for testset_path , testset_dataframe in test_datasets.items():\n",
        "            x_test = testset_dataframe['Tweet']\n",
        "            y_test = testset_dataframe['Sentiment']\n",
        "            y_pred = naive_bow_pipeline.predict(x_test)\n",
        "            id_preds={}\n",
        "            id_preds = {str(tweet_id): pred for tweet_id, pred in zip(testset_dataframe['TweetID'], y_pred)}\n",
        "            evaluate(id_preds,testset_path,classifier)\n",
        "\n",
        "\n",
        "        # Skeleton: Creation and training of the classifiers\n",
        "        elif classifier == 'svm' and features=='bow':\n",
        "          print('Training  '+ classifier + ' with ' + features + ' technique ')\n",
        "          svm_bow_pipeline = Pipeline([('vectorizer',CountVectorizer(stop_words='english')),('classifier',SVC(kernel='linear'))])\n",
        "          svm_bow_pipeline.fit(x_train, y_train)\n",
        "          #predicting on development set\n",
        "          y_pred_dev = svm_bow_pipeline.predict(x_dev)\n",
        "          print (classification_report(y_dev,y_pred_dev))\n",
        "          print('Time to evaluate Svm on test sets')\n",
        "          for testset_path , testset_dataframe in test_datasets.items():\n",
        "            x_test = testset_dataframe['Tweet']\n",
        "            y_test = testset_dataframe['Sentiment']\n",
        "            y_pred = svm_bow_pipeline.predict(x_test)\n",
        "            id_preds={}\n",
        "            id_preds = {str(tweet_id): pred for tweet_id, pred in zip(testset_dataframe['TweetID'], y_pred)}\n",
        "            evaluate(id_preds,testset_path,classifier)\n",
        "\n",
        "        elif classifier == 'logistic_regression' and features=='bow':\n",
        "          print('Training  '+ classifier + ' with ' + features + ' technique ')\n",
        "          lr_bow_pipeline = Pipeline([('vectorizer',CountVectorizer(stop_words='english')),('classifier',LogisticRegression(max_iter=1000))])\n",
        "          lr_bow_pipeline.fit(x_train, y_train)\n",
        "          #predicting on development set\n",
        "          y_pred_dev = lr_bow_pipeline.predict(x_dev)\n",
        "          print (classification_report(y_dev,y_pred_dev))\n",
        "          print('Time to evaluate logistic regression on test sets')\n",
        "          for testset_path , testset_dataframe in test_datasets.items():\n",
        "            x_test = testset_dataframe['Tweet']\n",
        "            y_test = testset_dataframe['Sentiment']\n",
        "            y_pred = lr_bow_pipeline.predict(x_test)\n",
        "            id_preds={}\n",
        "            id_preds = {str(tweet_id): pred for tweet_id, pred in zip(testset_dataframe['TweetID'], y_pred)}\n",
        "            evaluate(id_preds,testset_path,classifier)\n",
        "\n",
        "\n",
        "        elif classifier =='lstm':\n",
        "\n",
        "          print(' Training '+ classifier + ' with glove embeddings technique ')\n",
        "          words = dict()\n",
        "          def create_to_dict(d,filename):\n",
        "\n",
        "            with open(filename,'r',encoding='utf-8') as f :\n",
        "\n",
        "              for line in f.readlines():\n",
        "\n",
        "                line = line.split(' ')\n",
        "                try :\n",
        "\n",
        "                  d[line[0]] = np.array(line[1:] , dtype = float)\n",
        "                except:\n",
        "\n",
        "                  continue\n",
        "\n",
        "          create_to_dict(words,'C:/Users/Nikhil Wadhwa/glove.6B/glove.6B.50d.txt')\n",
        "          tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "          lemmatizer = WordNetLemmatizer()\n",
        "          def tweet_to_token_list(sentence):\n",
        "\n",
        "\n",
        "            tokens = tokenizer.tokenize(sentence)\n",
        "            tokens_lowercase = [t.lower() for t in tokens]\n",
        "            lemmatized_tokens = [lemmatizer.lemmatize(t) for t in tokens_lowercase]\n",
        "            imp_tokens = [t for t in lemmatized_tokens if t in words]\n",
        "            return imp_tokens\n",
        "          def tweet_to_word_vectors(tweet, word_dict=words):\n",
        "\n",
        "\n",
        "            processed_list_of_tokens = tweet_to_token_list(tweet)\n",
        "\n",
        "            vectors = []\n",
        "\n",
        "            for token in processed_list_of_tokens:\n",
        "\n",
        "\n",
        "              if token not in word_dict:\n",
        "\n",
        "                continue\n",
        "\n",
        "              token_vector = word_dict[token]\n",
        "              vectors.append(token_vector)\n",
        "\n",
        "            return np.array(vectors, dtype=float)\n",
        "          def df_to_X_y(dataframe_conv):\n",
        "\n",
        "\n",
        "            y = dataframe_conv['Sentiment']\n",
        "            le = LabelEncoder()\n",
        "            y=le.fit_transform(dataframe_conv['Sentiment'])\n",
        "\n",
        "            all_word_vector_sequences = []\n",
        "\n",
        "            for tweet in dataframe_conv['Tweet']:\n",
        "              tweet_as_vector_seq = tweet_to_word_vectors(tweet)\n",
        "              if tweet_as_vector_seq.shape[0] == 0:\n",
        "                tweet_as_vector_seq = np.zeros(shape=(1, 50))\n",
        "\n",
        "              all_word_vector_sequences.append(tweet_as_vector_seq)\n",
        "\n",
        "            return all_word_vector_sequences, y\n",
        "          X_train, y_train = df_to_X_y(train_data)\n",
        "          X_val , y_val = df_to_X_y(dev_data)\n",
        "          def pad_X(X, desired_sequence_length=242):\n",
        "            X_copy = deepcopy(X)\n",
        "            for i, x in enumerate(X):\n",
        "              x_seq_len = x.shape[0]\n",
        "              sequence_length_difference = desired_sequence_length - x_seq_len\n",
        "              padding = np.zeros(shape=(sequence_length_difference, 50))\n",
        "              X_copy[i] = np.concatenate([x, padding])\n",
        "            return np.array(X_copy).astype(float)\n",
        "          X_train = pad_X(X_train,desired_sequence_length=242)\n",
        "          X_val = pad_X(X_val)\n",
        "          model = Sequential([])\n",
        "\n",
        "          model.add(layers.Input(shape=(242, 50)))\n",
        "          model.add(layers.LSTM(64, return_sequences=True))\n",
        "          model.add(layers.Dropout(0.2))\n",
        "          model.add(layers.LSTM(64, return_sequences=True))\n",
        "          model.add(layers.Dropout(0.2))\n",
        "          model.add(layers.LSTM(64, return_sequences=True))\n",
        "          model.add(layers.Dropout(0.2))\n",
        "          model.add(layers.Flatten())\n",
        "          model.add(layers.Dense(3, activation='softmax'))\n",
        "          model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "          model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), batch_size=32)\n",
        "          predictions = model.predict(X_val)\n",
        "          predicted_classes = np.argmax(predictions, axis=1)\n",
        "          print(classification_report(y_val, predicted_classes, target_names=['positive', 'neutral', 'negative']))\n",
        "          for testset_path , testset_dataframe in test_datasets.items():\n",
        "            X_test, y_test = df_to_X_y(testset_dataframe)\n",
        "            X_test = pad_X(X_test)\n",
        "            y_test_pred = model.predict(X_test)\n",
        "            y_test_pred_class = np.argmax(y_test_pred,axis=1)\n",
        "            label_mapping = {0: 'positive', 1: 'neutral', 2: 'negative'}\n",
        "            y_test_pred_labels = [label_mapping[pred] for pred in y_test_pred_class]\n",
        "            testset_dataframe['TweetID'] = testset_dataframe['TweetID'].astype(str)\n",
        "            id_preds={}\n",
        "            id_preds = {str(tid): pred for tid, pred in zip(testset_dataframe['TweetID'], y_test_pred_labels)}\n",
        "            evaluate(id_preds,testset_path,classifier)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**lstm using pytorch and Glove word embeddings**"
      ],
      "metadata": {
        "id": "9rCE_ODc1pik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lstm using pytorch"
      ],
      "metadata": {
        "id": "U3b5BwV5xC6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KIVel5Qglyh",
        "outputId": "b9193902-e1da-4dc8-fa12-5405ba095070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to C:\\Users\\Nikhil\n",
            "[nltk_data]     Wadhwa\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOADING GLOVE EMBEDDINGS"
      ],
      "metadata": {
        "id": "Hqowwm47Me6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_embeddings_from_file(embeddings_file_path):\n",
        "    \"\"\"Load word embeddings from a given file.\n",
        "\n",
        "    Args:\n",
        "        embeddings_file_path (str): The file path to the pre-trained word embeddings.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are words and values are their corresponding embedding vectors.\n",
        "    \"\"\"\n",
        "    embeddings_dictionary = {}\n",
        "    with open(embeddings_file_path, 'r', encoding=\"utf8\") as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split()  # Improved naming and strip leading/trailing spaces\n",
        "            word = parts[0]\n",
        "            vector = np.asarray(parts[1:], dtype=\"float32\")\n",
        "            embeddings_dictionary[word] = vector\n",
        "    return embeddings_dictionary"
      ],
      "metadata": {
        "id": "IpehDp_vMeBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_embeddings= load_embeddings_from_file(os.path.join(os.getcwd(), 'glove.6B', 'glove.6B.100d.txt'))\n"
      ],
      "metadata": {
        "id": "PuTS9qiWjbyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_tokens = x_train.apply(lambda x: word_tokenize(x.lower()))\n",
        "x_val_tokens = x_dev.apply(lambda x: word_tokenize(x.lower()))"
      ],
      "metadata": {
        "id": "0aUJgl4GNodJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mapping these words and tokenising"
      ],
      "metadata": {
        "id": "xd9PHBDaRF_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_word_index_mapping(tokenized_texts):\n",
        "    \"\"\"\n",
        "    Create a mapping from words to unique integer indices.\n",
        "\n",
        "    Parameters:\n",
        "    - tokenized_texts: A list of lists, where each inner list contains tokens from a single document.\n",
        "\n",
        "    Returns:\n",
        "    - word_index: A dictionary mapping each unique token to a unique integer.\n",
        "    \"\"\"\n",
        "    word_index = {}\n",
        "    current_index = 1\n",
        "    for tokens in tokenized_texts:\n",
        "        for token in tokens:\n",
        "            if token not in word_index:\n",
        "                word_index[token] = current_index\n",
        "                current_index += 1\n",
        "    return word_index\n",
        "\n",
        "# Example usage:\n",
        "# Assuming X_train_tokens is a list of lists where each inner list is a tokenized text from your training data.\n",
        "# X_train_tokens = [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'sentence'], ...]\n",
        "\n",
        "word_index_mapping = create_word_index_mapping(x_train_tokens)\n"
      ],
      "metadata": {
        "id": "Sm9Mym_eQ-R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WWZ4vP7GhF2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "converting into sequence"
      ],
      "metadata": {
        "id": "o8JCj5aARquA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UddIRFsmh9Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_seq = [[word_index_mapping[token] for token in tokens if token in word_index_mapping] for tokens in x_train_tokens]\n",
        "x_val_seq = [[word_index_mapping[token] for token in tokens if token in word_index_mapping] for tokens in x_val_tokens]"
      ],
      "metadata": {
        "id": "7BWFLb8Bofsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4QCF4PfziBLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " max_length = max(max(len(seq) for seq in X_train_seq), max(len(seq) for seq in x_val_seq))"
      ],
      "metadata": {
        "id": "e-V9mDLlRuEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "padding step"
      ],
      "metadata": {
        "id": "R0o2sIxoTGIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sequences(sequences, max_length):\n",
        "    \"\"\"\n",
        "    Pad sequences to the same length.\n",
        "\n",
        "    Parameters:\n",
        "    - sequences: A list of lists where each inner list is a sequence of integers.\n",
        "    - max_length: The desired maximum length of each padded sequence.\n",
        "\n",
        "    Returns:\n",
        "    - padded_sequences: A numpy array of sequences padded to the same maximum length.\n",
        "    \"\"\"\n",
        "    # Initialize a numpy array with zeros of shape (number of sequences, max_length)\n",
        "    padded_sequences = np.zeros((len(sequences), max_length), dtype=int)\n",
        "\n",
        "    for idx, sequence in enumerate(sequences):\n",
        "        sequence_length = len(sequence)\n",
        "        if sequence_length >= max_length:\n",
        "            # If the sequence is longer than max_length, truncate it\n",
        "            padded_sequences[idx, :] = sequence[:max_length]\n",
        "        else:\n",
        "            # If the sequence is shorter, pad it with zeros at the beginning\n",
        "            padded_sequences[idx, -sequence_length:] = sequence\n",
        "\n",
        "    return padded_sequences"
      ],
      "metadata": {
        "id": "GWWFSIurR5nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rUT6jFgbiKKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = pad_sequences(X_train_seq,max_length)"
      ],
      "metadata": {
        "id": "14MNGvo7S0yA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "olLb290giYAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = pad_sequences(X_val_seq,max_length)"
      ],
      "metadata": {
        "id": "ryvXOA12TIyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ahxk32oEibQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = len(word_index_mapping) + 1\n",
        "# Dimension of the GloVe word vectors.\n",
        "embedding_dimension = 100\n",
        "\n",
        "# Initialize the embedding matrix with zeros.\n",
        "# Each row corresponds to a word vector for each word in the vocabulary.\n",
        "embedding_matrix = np.zeros((vocabulary_size, embedding_dimension))\n",
        "\n",
        "# Populate the embedding matrix with GloVe vectors.\n",
        "# If a word in our vocabulary is not found in GloVe, its vector is left as zeros.\n",
        "for word, index in word_index_mapping.items():\n",
        "  glove_vector = glove_embeddings.get(word)\n",
        "  if glove_vector is not None:\n",
        "    embedding_matrix[index] = glove_vector"
      ],
      "metadata": {
        "id": "wdxK7NlcTVrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dSfXqFO6ie8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentAnalyzerLSTM(nn.Module):\n",
        "    def __init__(self, vocabulary_size, embedding_dimension, lstm_hidden_dimension, output_dimension, pretrained_embedding_matrix):\n",
        "        super(SentimentAnalyzerLSTM, self).__init__()\n",
        "\n",
        "        # Store hyperparameters\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.lstm_hidden_dimension = lstm_hidden_dimension\n",
        "        self.output_dimension = output_dimension\n",
        "\n",
        "        # Embedding layer with pre-trained embeddings\n",
        "        self.embedding_layer = nn.Embedding(vocabulary_size, embedding_dimension)\n",
        "        self.embedding_layer.weight = nn.Parameter(torch.tensor(pretrained_embedding_matrix, dtype=torch.float32))\n",
        "        self.embedding_layer.weight.requires_grad = False  # Keep the embeddings fixed\n",
        "\n",
        "        # LSTM layer configured to expect input batch first\n",
        "        self.lstm_layer = nn.LSTM(embedding_dimension, lstm_hidden_dimension, batch_first=True)\n",
        "\n",
        "        # Fully connected layer to produce the output\n",
        "        self.output_layer = nn.Linear(lstm_hidden_dimension, output_dimension)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "\n",
        "    def forward(self, input_text_tensor):\n",
        "        # Convert input text to embeddings\n",
        "        text_embeddings = self.embedding_layer(input_text_tensor)\n",
        "\n",
        "        # Process the embeddings through the LSTM layer\n",
        "        _, (hidden_state, _) = self.lstm_layer(text_embeddings)\n",
        "\n",
        "        # Use the last hidden state as input to the fully connected layer\n",
        "        final_output = self.output_layer(hidden_state[-1])\n",
        "\n",
        "        return final_output"
      ],
      "metadata": {
        "id": "iN3DC_GlUBXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "coqLIaOOih4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = SentimentAnalyzerLSTM(vocabulary_size, embedding_dimension, lstm_hidden_dimension=128, output_dimension=3, pretrained_embedding_matrix=embedding_matrix)\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encode = label_encoder.fit_transform(y_train)\n",
        "y_dev_encode= label_encoder.transform(y_val)"
      ],
      "metadata": {
        "id": "DKzGL7vhU_4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "piLFUCe_i6bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_tensor = torch.tensor(x_train, dtype = torch.long)\n",
        "y_train_tensor = torch.tensor(y_train_encode, dtype = torch.long)\n",
        "x_dev_tensor = torch.tensor(x_val, dtype = torch.long)\n",
        "y_dev_tensor = torch.tensor(y_dev_encode, dtype = torch.long)\n"
      ],
      "metadata": {
        "id": "0eDOCIiQVHB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tensor_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_tensor_data, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "IN8Uo2cf1PnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss functions\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "Bn3KfvvQi7dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "lstm_model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI7XAanpW45O",
        "outputId": "623e6748-c633-4d0f-bbb2-bce2cab588d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SentimentAnalyzerLSTM(\n",
              "  (embedding_layer): Embedding(48654, 100)\n",
              "  (lstm_layer): LSTM(100, 128, batch_first=True)\n",
              "  (output_layer): Linear(in_features=128, out_features=3, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mMavdO54jFW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "  for inputs, labels in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    output = lstm_model(inputs)\n",
        "    loss = criterion(output, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
        "\n",
        "lstm_model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iy9oTr5yXc4l",
        "outputId": "1700158f-21a9-4e78-f390-1476b8775b4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.9494735598564148\n",
            "Epoch 2, Loss: 0.5372791290283203\n",
            "Epoch 3, Loss: 1.2389754056930542\n",
            "Epoch 4, Loss: 0.6985536217689514\n",
            "Epoch 5, Loss: 0.6781131625175476\n",
            "Epoch 6, Loss: 1.0243041515350342\n",
            "Epoch 7, Loss: 0.689394474029541\n",
            "Epoch 8, Loss: 0.951617419719696\n",
            "Epoch 9, Loss: 0.24189789593219757\n",
            "Epoch 10, Loss: 0.28844964504241943\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SentimentAnalyzerLSTM(\n",
              "  (embedding_layer): Embedding(48654, 100)\n",
              "  (lstm_layer): LSTM(100, 128, batch_first=True)\n",
              "  (output_layer): Linear(in_features=128, out_features=3, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    predictions_dev_lstm = lstm_model(x_dev_tensor_dev_tensor)\n",
        "    _, predicted_labels = torch.max(predictions_dev_lstm, 1)\n",
        "    print(classification_report(y_dev_tensor.numpy(), predicted_labels.numpy(), target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5eFC_a7X3K-",
        "outputId": "29ff4dcb-5772-4f5a-d812-ea96c416f758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.65      0.44      0.53       378\n",
            "     neutral       0.62      0.68      0.65       919\n",
            "    positive       0.65      0.69      0.67       703\n",
            "\n",
            "    accuracy                           0.64      2000\n",
            "   macro avg       0.64      0.60      0.62      2000\n",
            "weighted avg       0.64      0.64      0.63      2000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "15wUF0r01lS_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}